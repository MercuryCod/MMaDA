wandb:
  entity: null  # Set this to your wandb username/organization
  resume: 'auto'

experiment:
    project: "mmada-training-t2m-lora"
    name: "mmada-training-t2m-instruct-lora"
    output_dir: "mmada-training-t2m-instruct-lora"
    max_train_examples_t2i: 40000000
    max_train_examples_mmu: 40000000
    save_every: 10000
    eval_every: 5000
    generate_every: 2500
    log_every: 50
    log_grad_norm_every: 100
    # resume_from_checkpoint: "latest"  # Commented out for initial training
    val_every: 100
    max_val_examples_t2i: 2000

model:
    image_vq_model:
        type: "magvitv2"
        vq_model_name: "showlab/magvitv2"

    motion_vq_model:
        type: "vq"
        vq_model_name: "VQVAE"
        depth: 3
        dilation_growth_rate: 3
        nb_code: 512
        code_dim: 512
        output_emb_width: 512
        width: 512
        quantizer: "ema_reset"
        mu: 0.99
        activation: "relu"
        stride_t: 2
        vq_dir: ""
        resume_pth: "output/VQVAE/net_last.pth"

    mmada:
        tokenizer_path: "GSAI-ML/LLaDA-8B-Instruct"
        pretrained_model_path: "Gen-Verse/MMaDA-8B-MixCoT"
        w_clip_vit: False
        new_vocab_size: 135055  # Fixed: text (126349) + image (8192) + motion (512) + EOM (1) + PAD (1)
        llm_vocab_size: 126464
        image_codebook_size: 8192
        motion_codebook_size: 512
        num_vq_tokens: 256
        num_new_special_tokens: 0
        tie_word_embeddings: False

    lora:
        r: 32  # LoRA rank
        lora_alpha: 64  # LoRA alpha scaling
        lora_dropout: 0.1
        bias: "none"  # bias training strategy: "none", "lora_only" or "all"
        target_modules:  # Which modules to apply LoRA to
            - "q_proj"
            - "k_proj"
            - "v_proj"
            - "o_proj"
            - "gate_proj"
            - "up_proj"
            - "down_proj"
        train_embeddings: True  # Whether to train embeddings for new tokens

    gradient_checkpointing: True

dataset:
    gen_type: "t2m"
    params:
        dataset_name: "t2m"
        num_workers: 32
        down_t: 2
        stride_t: 2
        

    preprocessing:
        max_seq_length: 256 # for text tokens
        resolution: 256
        center_crop: False
        random_flip: False

optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 2e-4  # Higher LR for LoRA
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.01
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 2000  # Less warmup for LoRA
        min_lr_scale: 0.1

training:
    gradient_accumulation_steps: 2
    noise_type: "mask"
    batch_size_t2i: 7
    batch_size_lm: 2
    batch_size_mmu: 3
    batch_size_t2m: 8
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10086
    max_train_steps: 100000  # Less steps needed for LoRA
    overfit_one_batch: False
    cond_dropout_prob: 0.1
    min_masking_rate: 0.0
    label_smoothing: 0.0
    max_grad_norm: 1
    guidance_scale: 3
    generation_timesteps: 12
    t2i_coeff: 1.0
    lm_coeff: 0.1
    mmu_coeff: 0.5
    validation_seed: 42 