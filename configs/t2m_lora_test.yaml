wandb:
  entity: null
  resume: 'auto'
  mode: 'disabled'  # Disable wandb for testing

experiment:
    project: "mmada-training-t2m-lora-test"
    name: "mmada-t2m-lora-test"
    output_dir: "test_lora_output"
    max_train_examples_t2i: 40000000
    max_train_examples_mmu: 40000000
    save_every: 50  # Save more frequently for testing
    eval_every: 50  # Evaluate more frequently
    generate_every: 10  # Generate frequently to test
    log_every: 1    # Log every step
    val_every: 10   # Validate frequently
    max_val_examples_t2i: 2000

model:
    image_vq_model:
        type: "magvitv2"
        vq_model_name: "showlab/magvitv2"

    motion_vq_model:
        type: "vq"
        vq_model_name: "VQVAE"
        depth: 3
        dilation_growth_rate: 3
        nb_code: 512
        code_dim: 512
        output_emb_width: 512
        width: 512
        quantizer: "ema_reset"
        commitment_cost: 0.02
        usage_threshold: 0.0
        reset_prob: 0.0
        embed_grad_update: False
        activation: "relu"
        stride_t: 2
        vq_dir: ""
        resume_pth: "output/VQVAE/net_last.pth"
        mu: 0.99

    mmada:
        tokenizer_path: "GSAI-ML/LLaDA-8B-Instruct"
        pretrained_model_path: "Gen-Verse/MMaDA-8B-MixCoT"
        w_clip_vit: False
        new_vocab_size: 135055  # Fixed: text (126349) + image (8192) + motion (512) + EOM (1) + PAD (1)
        llm_vocab_size: 126464
        image_codebook_size: 8192
        motion_codebook_size: 512
        num_vq_tokens: 256
        num_new_special_tokens: 0
        tie_word_embeddings: False
        
    lora:
        r: 16
        lora_alpha: 32
        target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        lora_dropout: 0.1
        bias: "none"
        train_embeddings: True

dataset:
    dataset_type: 'humanml3d'
    params:
        code_path: './'
        dataset_name: 't2m'
        batch_size: 32
        num_frames: 64
        max_motion_length: 196
        unit_length: 4
        down_t: 2
        stride_t: 2
        max_text_len: 20
        min_motion_length: 40
        max_frames: 196
    preprocessing:
        text_tokenizer: 'GSAI-ML/LLaDA-8B-Instruct'
        max_seq_length: 256
        num_bins: 1000

training:
    seed: 42
    max_train_steps: 100  # Small number for testing
    batch_size_t2i: 16
    batch_size_mmu: 0
    batch_size_t2m: 2  # Small batch size for testing
    shuffle_buffer_size: 1000
    mixed_precision: "bf16"
    mask_schedule: "cosine"
    enable_tf32: True
    gradient_accumulation_steps: 1  # No accumulation for testing
    max_grad_norm: 1.0
    min_masking_rate: 0.0
    eval_mask_ratios: [0.125, 0.25, 0.5, 0.75]
    cond_dropout_prob: 0.1
    predict_all_tokens: False
    noise_type: 'mask'
    
    # Motion training specific
    motion_dim: 263
    motion_latent_dim: 512
    motion_ff_size: 1024
    motion_num_layers: 8
    motion_num_heads: 4
    motion_dropout: 0.1
    motion_activation: "gelu"

optimizer:
    name: "adamw"
    params:
        learning_rate: 5e-5  # Reasonable LR for LoRA
        beta1: 0.9
        beta2: 0.999
        weight_decay: 1e-6
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        warmup_steps: 10
        min_lr_scale: 0.1

# Use single GPU config instead of DeepSpeed for testing
accelerate:
    mixed_precision: "bf16"
    gradient_accumulation_steps: 1 